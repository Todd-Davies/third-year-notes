% Set the author and title of the compiled pdf
\hypersetup{
  pdftitle = {\Title},
  pdfauthor = {\Author}
}

% Part One

%\input{partone}

\section{Initial Value Problems}

An initial value problem is an ordinary differential equation with a
given value called the initial condition, which is the value of the
unknown function (that we're trying to model) at a given point in the
domain of the solution.

In the notes, the example of a chef leaving a large pot of soup to
cool is given. If the pot starts at $T_0 = 100$, and the desired
temperature of the soup is $T_e = 20$, then how long should the chef
leave it cooling?

We can't know this until we have the initial conditions, which is that
the ambient temperature is $T_\alpha = 10$, and the given point in the
solution domain; the chef left the soup for ten minutes and the
temperature was $T_i = 60$.

The cooling of the soup is modelled by $\frac{dT}{dt}$ (change in
temperature over time), and we can use Newton's law of cooling and
heating to change this to: $\frac{dT}{dt} = f(T - T_\alpha)$. Since we
know that the ambient temperature is larger than the starting
temperature ($T > T_\alpha$) and that the soup is actually cooling (as
evidenced by $T_i < T_0$), we know that $k < 0$.

We can work out the value of $k$ from the analytical solution:

\[
  T(t) = T_\alpha + (T_0 - T_\alpha)e^{-kt}
\]

By subbing in the values $t = 600, T(600) = 60, T_\alpha = 10$ we can
get $k$:

\[
  \begin{split}
  60 &= 10 + (100 - 10)e^{-600 \times k}\\
  50 &= 90e^{-600 \times k}\\
  \frac{5}{9} &= e^{-600 \times k}\\
  log(\frac{5}{9}) &= -600k\\
  k &= 9.796\times10^{-4}
  \end{split}
\]

To see how long the cook should leave the soup for, we can simply sub
in $k$ and sub in $20$ for the desired ending temperature:

\[
\begin{split}
  T(t) &= T_\alpha + (T_0 - T_\alpha)e^{-kt}\\
  20 &= 10 + 90e^{-9.796\times10^{-4}t}\\
  \frac{1}{9} &= e^{-9.796\times10^{-4}t}\\
  t &= \frac{log(\frac{1}{9})}{-9.796\times10^{-4}}\\
  t &= 2242s
\end{split}
\]

This was an easy problem though; often, an analytical solution (the
equation for $T(t)$) will not exist and we will need to solve the
problem numerically.

\subsection{IVP's: Linear Multistep Methods}

Basically we will have a function that takes in the current $x$ and
$y$ values and output the next $y$ value:

\[
  y' = f(x,y)
\]

Where $a < x < b$, and we know the `first' $y$ value $y(a) = \alpha$.

Linear multistep methods aim to emulate a target function using
$y'$. Given $y'$ (a function that produces the next value of a target
function $y' = f(x,y)$ for all $a < x < b$) and $y(a) = \alpha$, we
can use Taylor expansion to find the value of any point where $b$ is
greater than $a$:

% TODO: Explain Taylor Expansion

\[
\begin{split}
  y(x) &\approx y(a) + (b - a)y'(a)\\
       &\approx y(a) + (b - a)f(a, y(a))
\end{split}
\]

We want to move from $a$ to $b$ in small increments, otherwise any
inaccuracies in the taylor expansion will become quickly apparent, so
we define $h$ to be the step size between $x_n$ and $x_{n+1}$ (where
$x_k$ are the intermediate steps between $a$ and $b$), such that $h
= \frac{b - a}{N}$, where $N$ is the number of steps we want to take.

This means that $x_n$ = $a + nh$.

For Euler's method, we can describe:

\[
\begin{split}
  y_{n+1} &= y_n + (x_{n+1} - x_n)f(x_n, y_n)\\
          &= y_n + hf_n
\end{split}
\]

The Euler method only uses the previous point to generate the next
point, but in general, linear multistep methods use the previous $k$
steps to calculate the next value. As such, they are described by a
list of values $\alpha = (\alpha_1, \dots, \alpha_k), \beta =
(\beta_1,\dots,\beta_k)$ ($\alpha_k$ is always 1), such that:

\[
  \sum\limits^k_{i=0}\alpha_iy_{n+i} = h \sum\limits^k_{i=0}\beta_if_{n+i}
\]

The designer of the method determines the coefficients
$\alpha_{0,\dots,k-1}$ ($alpha_k$ is 1 remember) and $\beta_{0, k}$,
and they will balance the accuracy of the approximation with the need
to make the method easy to apply.

If $\beta_k = 0$, then the method is said to be explicit, since the
formula can directly compute $y_{n + k}$, whereas if $\beta_k \neq 0$,
then the $y_{n+k}$ depends on $f(x_{n+x},y_{n+k})$, which means an
equation must be solved to find $y_{n+k}$:

\[
  y_{n+k} = h\beta_kf(x_{n+k},y_{n+k})
              + h\sum\limits^{k-1}_{i=0}\beta_if_{n+i}
              - \sum\limits^{k-1}_{i=0}\alpha_iy_{n+i}
\]

Lets use the Euler method applied to the soup problems as an
example. Since $\frac{dT}{dt} = -k(T - T_a)$\footnote{The change in
temperature over time is equal to the temperature minus the ambient
temperature multiplied by the cooling coefficient.},
$f(x_{n+1},y_{n+1}) = -k(T_n - T_a)$.

\begin{center}
\begin{tabular}{ll}
  Forward Euler & $T_{n+1} = T_n + h(-k(T_n - T_a))$\\
  Backward Euler & $T_{n+1} = T_n + h(-k(T_{n+1} - T_a))$\\
\end{tabular}
\end{center}

Since we know the starting temperature of the soup ($T_0 = 100$), we
can simply iterate towards the end value with the forward Euler
method. Lets say $h = 20$ and $k = 9.796\times10^{-4}$ as we worked
out earlier:

\[
\begin{split}
  T_0 &= 100\\
  T_1 &= 100 + 20(-9.796\times10^{-4}(100 - 10)) = 98.237\\
  T_2 &= 98.237 + 20(-9.796\times10^{-4}(98.237 - 10)) = 96.508\\
  &\vdots\\
  T_{111} &= \dots = 20.009\\
  T_{112} &= \dots = 19.813\\
\end{split}
\]

Since we did $112$ iterations before we dropped below the target
temperature of $20$, and $h = 20$, we know the time is between $2220$
and $2240$ seconds. Since we previously calculated the actual time it
would take ($2242$ seconds), we're pretty close to the right
answer. We probably got a bit of inaccuracy because the forwards Euler
method is unstable (we'll get to that later). If we decreased $h$,
then we would get a better answer (in fact, when $h=1$, we get $~2242$
seconds).

We can also use the backward Euler method. To do this we need to
rearrange the method to give $T_{n+1}$ so that it is not a recursive
definition:

\[
\begin{split}
  T_{n+1} &= T_n + h(-k(T_{n+1} - T_a))\\
          &= T_n - hk(T_{n+1} - T_a)\\
  \frac{T_{n+1}}{-hk} &= \frac{T_n}{-hk} + T_{n+1} - T_a\\
  \frac{T_{n+1}}{-hk} - T_{n+1} &= \frac{T_n}{-hk} - T_a\\
  T_{n+1} + khT_{n+1} &= T_n + hkT_a\\
  T_{n+1}(1 + hk) &= T_n + hkT_a\\
  T_{n+1} &= \frac{1}{1 + hk}(T_n + hkT_a)
  \end{split}
\]

So lets apply the backwards Euler method with $h=20$:

\[
\begin{split}
  T_0 &= 100\\
  T_1 &= \frac{1}{1 + 20\times9.796\times10^{-4}}(100 +
  20\times9.796\times10^{-4}\times10) = 98.271\\
  T_2 &= \frac{1}{1 + 20\times9.796\times10^{-4}}(98.271 +
  20\times9.796\times10^{-4}\times10) = 96.574\\
  &\vdots\\
  T_{112} &= 20.047\\
  T_{113} &= 19.854\\
\end{split}
\]

Therefore we can see that the backwards Euler method says that the
temperature reaches $20$ degrees bewteen $2240$ and $2260$ seconds
($112$ and $113$ times $h$). If we decreased the step size $h$, we
could get a more accurate calculation (when $h=1$ we get $2244-2245$
seconds). The benefit of the backwards method is that it is
unconditionally stable (i.e. errors in the input between successive
iterations will diminish as the algorithm
executes)\footnote{\url{http://mathworld.wolfram.com/NumericalStability.html}}.

\subsubsection{Consistency of Linear Multistep Methods}

We have seen how to construct linear multistep methods, and had a look
at how to make them stable. Now we must examine their consistency,
which is the property that as $h$ decreases, the truncation error can
be made to approach zero; i,e, as $h$ approaches
$\frac{1}{\infty}$, we approach the true solution of the equation.

To find if a method is consistent, we must first find its order. This
is done by defining the constants $C_0, \dots, C_j$:

\[
\begin{split}
C_0 &= \alpha_0 + \dots + \alpha_k\\
C_1 &= \alpha_1 + 2\alpha_2 + \dots + k\alpha_k - (\beta_0 + \dots
+ \beta_k)\\
C_j &= \frac{1}{j!}(\alpha_1 + 2^j\alpha_2 + \dots + k^j\alpha_k)
- \frac{1}{(j-1)!}(\beta_1 + 2^{j-1}\beta_2 + \dots + k^{j-1}\beta_k)
\end{split}
\]

A linear multistep method is of order $p$ if $C_{p+1} \neq 0$. In
other words $C_0 = C_1 = \dots = C_p = 0, C_{p+1} \neq 0$. If the
method is of order $p \geq 1$ then it is consistent. This can also be
expressed as the sum of the alphas being $0$, or the sum of the
product of the index and the alpha being equal to the sum of the
betas:

\[
  \sum\limits^k_{i=0}\alpha_i = 0, \sum\limits^k_{i=0}i\alpha_i
    = \sum\limits^k_{i=0}\beta_i
\]

\subsubsection{Stability of Linear Multistep Methods}

A method is stable if its first characteristic polynomial does not
have zeros with a modulus greater than one, and if all the zeros with
a modulus equal to one are simple. This sounded very abstract to me,
but after talking to Milan about it, I'll give my best shot at what it
means:

We must define the first characteristic polynomial to be:

\[
\begin{split}
  p(\zeta) &= \alpha_k\zeta^k + \alpha_{k-1}\zeta^{k-1} + \dots
  + \alpha_0\zeta^0\\
           &= \alpha_k\zeta^k + \alpha_{k-1}\zeta^{k-1} + \dots
  + \alpha_1\zeta + \alpha_0\\
\end{split}
\]

Now, we need to find the roots of this polynomial (i.e. where
$p(\zeta) = 0$), which will give us a set of values
$\zeta_1,\dots,\zeta_n$ (usually there are only a few values). Some of
these values can be complex numbers (e.g. if $\zeta = a + ib$ is a
solution, then $\zeta = a - ib$ is too). To calculate the modulus of
one of these, you get the actual value if the number is real, or for
complex numbers, you can do $|\zeta_n| = \sqrt{a^2 + b^2}$.

\subsubsection{Optimal Methods}

An optimal method is one that is both stable, and has an order of $k +
2$ (where $k$ is the number of terms it has, e.g. the Euler method has
$k = 1$, therefore the Euler method is not optimal).

The First Dahlquist Barrier states that a method cannot attain an
order of convergence greater than $k + 1$ is $k$ is odd or $k + 2$ if
$k$ is even.

\subsubsection{Examples of Analysing Linear Multistep Methods}

\begin{description}
\item \textbf{Example One:}\\

Lets examine the Euler method first, defined as $y_{n+1} = y_n +
hf_n$. We first need to get it into the form where all the $y$'s are
on one side, and all the $f$'s are on the other side (this is easy):

\[
  y_{n+1} - y_n = hf_n
\]

Now we can simply read off the values of $\alpha$ and $\beta$:

\[
\begin{split}
  \alpha_0 &= -1\\
  \alpha_1 &= 1\\
  \beta_0 &= 1\\
  \beta_1 &= 0
\end{split}
\]

Using these, we can calculate the values of the constants:

\[
\begin{split}
C_0 &= \alpha_0 + \alpha_1 = -1 + 1 = 0\\
C_1 &= \alpha_1 - \beta_0 - \beta_1 = 1 - 1 - 0 = 0\\
C_2 &= \frac{1}{2}\alpha_1 - \beta_1 = \frac{1}{2} - 0
= \frac{1}{2} \neq 0
\end{split}
\]

Since $C_2$ is not $0$, then the method is consistent and of the first
order. To calculate the stability, then we must find the first
characteristic polynomial:

\[
  p(\zeta) = \alpha_1\zeta^1 - (\alpha_0 \zeta^0) = \zeta - 1
\]

Now we can solve for $\zeta$:

\[
\begin{split}
  p(\zeta) &= 0\\
  \zeta - 1 &= 0\\
  \zeta &= 1\\
\end{split}
\]

Since $\zeta = 1$ (which satisfies the conditions we said before), we
know that it is stable. Since it is stable and it is consistent, then
it is convergent.

\item \textbf{Example Two:}\\

For the next example, lets look at the equation:

\[
  y_{n+2} - y_{n+1} = \frac{h}{3}(3f_{n+1} - 2f_n)
\]

Lets see if it is consistent. To do this we must compute the
constants:

\[
  \begin{split}
    C_0 &= 0 - 1 + 1 = 0\\
    C_1 &= -1 + 2(2) - (0 + \frac{3}{3} - {2}{3}\\
        &= 3 - \frac{1}{3}\\
        &= 2\frac{2}{3}
  \end{split}
\]

Since $C_1 \neq 0$, the method is not consistent (and therefore cannot
be convergent either). Next we can see if it is stable, but getting
the first characteristic polynomial:

\[
\begin{split}
  p(\zeta) &= \alpha_2\zeta^2 + \alpha_1\zeta + \alpha_0\\
           &= \zeta^2 - \zeta\\
  p(\zeta) &= 0\\
  \zeta = 0&, \zeta = 1
\end{split}
\]

Therefore we can see that the method is stable (since the roots are
between 0 and 1).

\item \textbf{Example Three:}\\

We have:

\[
  y_{n+2} + 4y_{n+1} - 5y_n = \frac{2}{h}(8f_{n+1} + 4f_n)
\]

The consistency would be:

\[
\begin{split}
  C_0 &= 1 + 4 - 5 + 0 = 0\\
  C_1 &= 2(1) + 4 - (4 + 2) = 0\\
  C_2 &= \frac{1}{2!}(-5 + 2^2(4) + 3^2(1)) - \frac{1}{1!}(2 +
  2^1(4))\\
      &= \frac{1}{2}(4(1) + 4) - 4\\
      &= 0\\
  C_3 &= \frac{1}{3!}(8(1) + 4) - \frac{1}{2!}(4)\\
      &= \frac{1}{6}(12) - 2\\
      &= 0\\
  C_4 &= \frac{1}{4!}(16(1) + 4) - \frac{1}{3!}(4)\\
      &= \frac{1}{24}(20) - 1.5\\
      &= -\frac{2}{3}
\end{split}
\]

Since it is of order $3$, the method is consistent.

To find the stability, we must find the first characteristic
polynomial:

\[
\begin{split}
  p(\zeta) &= \zeta^2 + 4\zeta - 5\\
  p(\zeta) &= 0\\
  \zeta = 5&, \zeta = 1
\end{split}
\]

Since $\zeta = 5$, the method is not stable, and is therefore not
convergent.

\end{description}

% Part Two

\section{Optimisation and nature inspired algorithms}

To start, lets recap some stuff you should already know; a minima is a
point of a function where all points in its vicinity have a higher
value than itself, and a maxima is the opposite; a point where all
points in its vicinity have a lower value than itself.

Since we can invert a function by putting a minus in front of it, we
don't really need to differentiate between maxima and minima, since we
can always express one in terms of the other. We can however
differentiate between \textit{global optimums}, which are the best
values for the entire domain, and \textit{local optimums} are best in
some bounded region.

One dimensional functions are easiest to visualise; they are just a
graph, where the y value is the value of the function and the x value
is the input. As the dimensionality increases, the functions get
harder to visualise; 2d functions are visualised on a 2d graph, where
the intensity of the colour in each square represents the value of the
function at that coordinate.

An optimisation problem is one where we attempt to maximise or minimise a
function. This involves finding the input that will produce the largest or
smallest value. Optimisation problems are really search problems; given a
domain, find an input that produces the most desirable output.

If we know exactly what the function we're trying to optimise is
(i.e. we have an equation), then it's an explicit problem, but if we
just have some inputs to an unknown function, and their corresponding
output values, then it's a black box problem.

Similarly, there are two different approaches to solving these problems:

\begin{description}
  \item \textbf{Single solution}:\\
   This is where there is a single candidate solution that is incrementally 
   improved by the algorithm throughout the procedure.
  \item \textbf{Population based solution}:\\
    This is where there is a set of candidate solutions (the
    population) and an iterative operation combines and mutates the
    best ones, or increments the whole population to improve the
    quality of the population.
\end{description}

All of the optimisation methods follow a cycle:

\begin{itemize}
  \item Guess some parameters for the initial solution
  \item While we're not satisfied:
  \begin{itemize}
    \item Evaluate how the current parameters perform
    \item If we're satisfied, then we've finished
    \item Otherwise, determine new parameters based on the current ones
      and their evaluation.
  \end{itemize}
\end{itemize}

We can use derivatives to try and find maxima and minima in a
function. If we do not have an explicit function for the derivative,
then we can calculate them using \textbf{finite differences}, of which
two approaches are presented here:

\begin{description}
  \item \textit{Forward difference}:\\
    \[
      f'(x) \approx \frac{f(x + h) - f(x)}{h}
    \]
  \item \textit{Central difference}:\\
    \[
      f'(x) \approx \frac{f(x + h) - f(x - h)}{2h}
    \]
\end{description}

Here, the $h$ parameter is the range over which we're calculating the
differential.

\subsection{Minimising univariate functions}

For one dimensional functions, we can use two approaches:

\begin{itemize}
  \item Interval reduction, where we iteratively reduce the range of values that
  we think the optimum is inside.
  \item Interpolation, where we try to find an approximate function and use the
  optimum of that function to iterate to find a better approximate function.
\end{itemize}

\subsubsection{Bisection algorithm}

This is the easiest algorithm, we start with the full range in the domain, and
then cut it in half based on the value of the differential at the midpoint at
the range.

\begin{verbatim}
  bisection(f'(x), a, b, e) {
    while (b - a >= e) {
      c = (a + b) / 2;
      if (f'(c) < 0) a = c;
      else if(f'(c) > 0) b = c
      else return c;
    }
  }
\end{verbatim}

\subsubsection{Quadratic interpolation algorithm}

Here, we make a quadratic function that goes through three points $a,
b$ and $c$, then we either drop $a$ or $b$ depending on which way we
should move the quadratic function. Here, \textbf{we do not need the
differential} which is good since we won't always have it!

Please look at the lecture notes for psudo code here.

% TODO: I don't think this is right...
%% \begin{verbatim}
%%   quadInterpolate(f'(x), a, b, e) { 
%%     i = 1;
%%     c1 = (a + b) / 2;
%%     c2 = compute(a, b, c1);
%%     while (|c2 - c1| <= e) {
%%       cn = 0;
%%       l = min(c1, c2);
%%       r = max(c1, c2);
%%       if (f(r) < f(l)) {
%%         a = l;
%%         cn = r;
%%       } else {
%%         b = r;
%%         cn = l;
%%       }
%%       i += 1;
%%       cn = compute(a, b, c1);
%%       c1 = c2;
%%       c2 = cn;
%%     }
%%   }
%% \end{verbatim}

\subsubsection{Stopping criteria}

Eventually, we will have to stop our optimisation algorithms, but some of them
will run indefinitely. We need criteria to make them stop after a sane amount of
time. The simple approach is to stop after $n$ iterations, and is applicable to
all iterative functions. The more clever approach is to only stop when we reach
a certain threshold, i.e. when a method converges asymptotically on an optimum.

\subsection{Minimisation of multivariate functions}

There are two different classes of minimisation methods for multivariate
functions:

\begin{description}
  \item \textbf{Based on derivatives}:\\
    Moves are determined based on information from derivatives of the 
    multivariate function.

    A \textit{directional derivative} is a derivative indicating the rate of
    change in a specific dimension:

    % TODO: Give an example of this, since it's really quite vague.

    \[
      \Phi'_r = lim_{h \rightarrow 0}\frac{f(x + hr) - f(x)}{h}
    \]

    Where $r$ is the direction. If $\Phi'_r$ is positive, then the direction is
    ascending, if it's negative, then the direction is descending and if it's
    equal to zero, then there is no slope.

    Finding the gradient of a vector of a multidimensional function $f(x)$ (and
    hence the vector of all partial derivatives) is:

    \[
      \frac{\delta f}{\delta x_j} =
        lim_{h \rightarrow 0} \frac{f(x + he_j) - f(x)}{h}
    \]

    $e_j$ is the unit vector in the direction of axis $j$, so to construct the 
    whole gradient for the vector, we need to do:

    \[
      \Delta f(x) = \begin{bmatrix}
        \frac{\delta f}{\delta x_1}(x) & \frac{\delta f}{\delta x_2}(x) & \dots,
        \frac{\delta f}{\delta x_n}(x)
      \end{bmatrix}^{T}
    \]

    The \textbf{Hessian} is the matrix containing the second order partial
    derivatives, and is always of size $N \times N$:

    \[
      Hf(x) = \begin{bmatrix}
        \frac{\delta^2 f}{\delta x_1 \delta x_1}(x) & \dots &
          \frac{\delta^2 f}{\delta x_1 \delta x_n}(x)\\
        \vdots & \ddots & \vdots\\
        \frac{\delta^2 f}{\delta x_n \delta x_1}(x) & \dots &
          \frac{\delta^2 f}{\delta x_n \delta x_n}(x)\\
      \end{bmatrix}
    \]

    Most algorithms have some kind of \textit{parameter update scheme}, where
    they will move in a given direction $r$ for a length of $\alpha$ every
    iteration,  and $r, \alpha$ change based on the current position:

    \[
      x_{i+1} = x_i + r_i\alpha_i
    \]

    \subsubsection{The alternating variable method}

    A conceptually easy method, you start from an arbitrary position $x$, and
    then for each dimension $d$:

    \begin{itemize}
      \item Find the differential for that dimension
      \item Find the minimum according to that differential
      \item Move the value of $x_d$ to be equal to that minimum.
    \end{itemize}

    Then repeat for all dimensions until no progress is made.

    %TODO: Insert picture

    \subsubsection{Newton's method}

    As soon as Newton invented calculus, he also started inventing methods for
    finding minima. This method requires a function to be twice differentiable:

    \[
      x_{i+1} = x_i - \alpha\frac{\Delta f(x_i)}{Hf(x_i))}
    \]

    %TODO: Give an example of this...

    The trouble with Newton's method is that it will only converge if the
    initial point is close to the solution, and the memory requirements for the
    Hessian matrix is $n^2$.

  \item \textbf{Direct search}:\\
    Moves are determined using methods other than derivatives, for example,
    using geometric concepts.

    \subsection{Simplex methods}

    If you did Advanced Algorithms 1, you're either groaning now or you're
    jumping for joy at being able to skip a section. However, these aren't the
    simplex methods you learned about last semester.

    For this, you construct a simplex (a shape of $n+1$ vertices in $n$
    dimensions), and then observe that you can always form a new simplex from an
    existing one by adding a new point.

    The \textbf{Spendley, Hext and Himsworth method} is like so:

    \begin{enumerate}
      \item Create $n+1$ vectors for a regular simplex (where all sides are the
      same length).
      \item For each vector, the value of the objected point is calculated for
      that point.
      \item The vector with the highest point is removed and substituted by a 
      new one.
      \item If the worst vector is also the most recently introduced one, then
      the next worst one is used.
      \item The newly substituted vector is the mirror image of the old one
      along the axis of the remaining vectors. %TODO: Equation
      \item When the simplex simply rotates around a point, then
      the vertex in the middle is the minimum.
    \end{enumerate}

    The maximum age of a vector is $1.65n + 0.05 n^2$, and if a vertex exceeds
    age $a$, then the search stops, or is restarted using a smaller simplex.
    If the search was restarted, then the initial (smaller) simplex is started
    from the site of the rotating.

    The \textbf{Nelder and Mead method} is similar, though the simplices are no
    longer regular (though they can be). It involves having rules that either
    expand, contract or reflect the simplex (or any combination of them), which
    leads to quicker results and better approximations of the solution.

    Where $y^{(r)}_0$ is the best value in the previous simplex, and
    $y^{(r)}_{n-1}$ is the worst, the new simplex results from:

    \begin{description}
      \item $y^{(r)}_0 < y^{(r+1)}_{i} < y^{(r)}_{n-1}$:\\
        The simplex is reflected like in the SHH method.
      \item $y^{(r+1)}_{i} < y^{(r)}_{0}$:\\
        The simplex is reflected and expanded.
      \item $y^{(r+1)}_{i} > y^{(r)}_{n-1}$:\\
        The simplex is reflected and contracted.
    \end{description}
\end{description}

% Lecture 3 of part 2

\subsection{Optimisation and nature inspired algorithms}

As we have seen, a global optimum is the best set of admissible conditions to
achieve an objective (i.e. minimising the function) under a set of constraints.

Finding such an optimum is non-trivial, mainly because the search space is often
continuous, so there are an infinite number of parameters to test. The following
algorithms try to find global optimums:

\begin{description}
  \item \textbf{Grid Search}:\\
  We overlay a grid onto the search space (in as many dimensions as we need to),
  and test each point on the grid. We can change the resolution of the grid by
  making the space between the grid lines larger or smaller. The algorithm
  scales exponentially in the number of dimensions, but linearly in the number
  of points tested (dependent on the grid resolution), therefore the runtime is
  $O(n^d)$.

  The quality of the solution depends on the density of the grid, but getting a
  high quality solution is usually impractical, since the search space grows so
  quickly (even in two dimensions, it is quadratic).

  \item \textbf{Random search}:\\
  Here, we simply generate $n$ random points and sample the objective function
  at these points. The point that gives the best value for the objective
  function is remembered for possible later use (since that's the optimum).
  Obviously this algorithm runs in $O(n)$ time, and its quality depends on the
  number of random points chosen. It is possible that the random search method
  could outperform all the other methods if it got lucky, though this is
  unlikely.

  Due to its ease of implementation, random search is often used as a reference
  for other methods.

  \item \textbf{Multistart}:\\

  Here, we guess $n$ random points again, but for each one, we find the local
  minimum. This improves the quality (since there's more chance of finding the
  global minimum, since it's definitely a local minimum too), but it can degrade
  performance.

  This method often visits the same local minima multiple times (if
  optimisations aren't made to avoid doing this) so it can be inefficient.

  \item \textbf{Random walk}:\\
  We can generate a single random start point, but then generate subsequent
  points by choosing a random direction and length for a vector and adding that
  to the current point.

  \item \textbf{Using nature to help minimise}:\\
  If a crystal is being formed, and it cools slowly, then the atoms in the
  crystal will vibrate around until they are in a local minima, which results in
  some cool shapes. We can use techniques to emulate this to find minima in our
  functions.

  Furthermore in genetics, non-optimal genes are `selected out' of the
  population by natural selection and evolution, leading to a genotype that is
  optimum for the environment.

  The remaining algorithms use nature as inspiration:

  \begin{description}
    \item \textbf{Metropolis algorithm}:\\
      This is an algorithm that emulates an ensemble of particles in equilibrium
      at a certain temperature. It uses the Boltzmann probability density
      function:

      \[
        p.d.f = e^{\frac{-E}{k_BT}}
      \]

      To give the probability that a certain particle configuration with an 
      energy $E$ has a certain temperature $T$.

      In nature, perfect crystals are formed by the cooling down from being
      molten very slowly so that the material can reach an equilibrium at each
      temperature. If the cooling is too fast, then an equilibrium will not be
      reached at each temperature, and the crystal will have imperfections.

      Local optimisation algorithms have parallels with letting crystals cool
      too fast!

      The algorithm is as follows:

      \begin{itemize}
        \item Start with a random position for an atom $x^0$
        \item Create a small random displacement to obtain $x^1$ and calculate
        the difference in energy $\Delta E = E^1 - E^0$.
        \item If $\Delta E < 0$ accept the new position, otherwise accept it
        with a probability of $P(\Delta E) = e^{\frac{-\Delta E}{k_BT}}$
        \item Iterate a lot of times, simulating the thermal motion of particles 
        in a heat bath of temperature $T$.
      \end{itemize}

      The Metropolis algorithm works because it lets the energy of the particle
      increase even though that's a `step back' in terms of optimisation.
      Sometimes the algorithm needs to get out of a local minima in order to
      find the global optimum.
    \item \textbf{Simulated Annealing}:\\
      This takes advantage of the fact that we can see the energy of a particle
      was similar to the value of an objective function we're trying to
      maximise, and the coordinates that it is at as similar to the parameters
      of the function we're finding an optimum for.

      The only other variable is the temperature, which acts as a control
      parameter with the same units as the objective function. Simulated
      annealing starts by melting the objective function at a high temperature,
      then using the Metropolis algorithm to calculate the equilibrium of the
      objective function at temperature decreases. Here's the algorithm:

      \begin{itemize}
        \item Start with a high temperature $T^0$ at some random position
        $x^{(0, T^0)}$.
        \item Apply the Metropolis algorithm to determine the average 
        equilibrium value of the objective function and parameter values
        $x^{(Ex, T^0)}$.
        \item keep reducing $T$ and repeating the previous step, and only stop
        when the function freezes; you've reached the global minimum.
      \end{itemize}

      Note that the start position is irrelevant, since there are many
      iterations required for each temperature, and an equilibrium is found for
      each. The start temperature, $T^0$ is very important though; if its too
      low, then a local minimum will be found, and if its too high, then the
      algorithm will take too long.

      \marginpar{Random and grid searches are also asymptotically complete, but
      they converge at much slower rates.}

      Simulated annealing can be reformulated computationally as a Markov Chain,
      and a proof has been given that states the algorithm will converge to a
      global optimum in infinite time, which makes simulated annealing
      \textit{asymptotically complete}.

    \item \textbf{Press' modification}:\\
      In 1989, Press et al. suggested that each temperature cycle should last
      for a predetermined number of iterations $N$. After each cycle, the
      temperature should be reduced by a constant factor $p$:

      \[
        T^{n+1} = pT^n~~~< 0\leq p \leq 1
      \]

      If after a cycle has finished, there have been no successful moves, then
      the algorithm stops. If we increase $N$, then the accuracy increases, but
      the execution time increases faster too. Similarly if we increase $p$, the
      solution will improve (and the reliability too), but the rate of cooling
      drops so it takes longer.

      There is also a slight modification to the displacement step; a single
      parameter is changed to a random value within the boundaries of the
      parameter.

    \item \textbf{Corana's modification}:\\
      Corana et al. suggested that the displacement step should be based off a
      vector; each parameter is changed according to the same coordinate of a
      vector $v_i$. After $N_s$ rounds, $v_i$ is changed so half the new
      candidate parameters are changed. Just like Press' modification, after
      $N_r$ rounds, the temperature is reduced.

      As a result, the annealing will adapt to the shape of the function, but
      it is also slower; $N$ in Press' algorithm is equal to $N_s \times N_r$ in
      Corana's algorithm.
  \end{description}

\end{description}

% Lecture 4 of part 2

\subsection{Evolutionary algorithms}

Natural populations evolve by having variation amongst the members of the
population and having selection (or unfit members of the population dying before
they get chance to reproduce). Evolutionary algorithms are a class of
optimisation algorithms that evolve candidate solutions as a group (ensemble)
rather than looking at one solution at a time.

If you took a biology course at some point, then you may be able to figure this
out, but there is a mapping from biology terminology to CS terminology:

\begin{itemize}
  \item Gene  - Parameter
  \item Chromosome - All parameters
  \item Individual - Candidate solution
  \item Generation - One iteration
  \item Fitness - Value of the objective function
\end{itemize}

Evolutionary algorithms evolve their populations. This means there will be $n$
points in the parameter space, and each iteration will drop some of the points
and create new ones (in hopefully better locations). Each individual in the 
population is a single candidate solution, and is stored as a
\textit{chromosome}. This is a sequence of bits, split into $m$ sections called
\textit{genes}. As described above, each gene maps onto a parameter.

\begin{center}
\fbox{
\begin{minipage}{0.5\textwidth}
\textbf{Chromosome:}\\
\texttt{1100110 1010101 1010111 0101101 1011010}\\
\textbf{Genes (each one is a parameter)}\\
\texttt{102 85 87 45 90}\\
\textbf{Alternately, floating point parameters}\\
\texttt{4.48 2.20 8.80 3.0 1.11 4.12}
\end{minipage}
}
\end{center}

Fogel's algorithm for evolutionary programming is as such:

\begin{enumerate}
  \item Generate a random initial population of $n$ individuals
  \item Calculate the fitness of each individual in the population (using the
  objective function)
  \item Each individual from the current population generates and offspring by
  copying its own genes
  \item Mutate each gene for each chromosome in the offspring by a small 
  variance.
  \item Put the offspring in a new population, $2n$ large
  \item Probabilistically select $n$ individuals as a function of fitness to be
  removed (now we have a population of $n$ again).
  \item Go back to step 2, or stop.
\end{enumerate}

In this algorithm, we described mutation (in step 4) and selection (in step 6).
For mutation, we add a small random number to the original value of a gene.

For selection, we want to select $n$ individuals, where better
individuals have a greater chance of remaining in the population. A
roulette wheel is a manual way of doing this (each individual has a
probability of being picked according to their fitness), but
stochastic ranking (e.g. a sort that has a chance of being wrong) or a
tournament where each individual is compared against a small number of
others and receives a score based off of its performance; the $n$
lowest scoring individuals are pruned.

Holland and DeJong's Genetic algorithm is as follows. It encodes parameters in
binary, and genes are therefore strings of binary digits.

\begin{enumerate}
  \item Generate $n$ random individuals for the population
  \item Calculate the fitness of each individual in the population
  \item Choose two parent individuals from the current population as a 
  probability of their fitness
  \item Cross them over (see below) at a random locus to produce two offspring
  \item Mutate each locus (gene) in the offspring with a small probability
  \item Put the offspring in the new population
  \item Go to step two, or stop.
\end{enumerate}

The mutation operation flips bits in a gene with a small probability. The cross
over operator swaps the latter halves of two chromosomes at a random index.

\begin{center}
\begin{tikzpicture}
  \path [fill=gray] (0.2,-0.2) rectangle (1.02,0.2);
  \path [fill=gray] (2.2,-0.2) rectangle (3.02,0.2);
  \path [fill=gray] (0.2,0.8) rectangle (1.02,1.2);
  \path [fill=gray] (2.2,0.8) rectangle (3.02,1.2);
  \node at (1,1) {\texttt{01101000}};
  \node at (3,1) {\texttt{01010010}};
  \node at (1,0) {\texttt{01011000}};
  \node at (3,0) {\texttt{01100010}};
  \draw [->] (0.5,0.8) -- (2.5,0.2);
  \draw [->] (2.5,0.8) -- (0.5,0.2);
\end{tikzpicture}
\end{center}

The differences in the two algorithms are as follows:

\begin{center}
  \begin{tabularx}{0.8\textwidth}{X|X}
    \textbf{Evolutionary programming} & \textbf{Genetic algorithms}\\ \hline
    Genes are encoded as floating point numbers & Genes are encoded in binary.\\
    Mutation is addition of a random value & Mutation is bit flipping.\\
    Asexual reproduction (two parent makes one child) & Sexual reproduction (two
    parents make two children.)\\
    No cross over & Cross over by swapping subsections of the chromosome.
  \end{tabularx}
\end{center}

Rechenberger and Schwefel made another algorithm. In this algorithm,
each gene has an associated standard deviation which affects how it
mutates. This is an innovation because in the standard approach, the
sizer of each mutation is the same for all genesand is constant
throughout the execution of the algorithm. Here, the standard
deviation for each gene is updated throughout the algorithm, and
henceforth mutations are different sizes for each gene and changes as
the algorithm progresses:

\begin{enumerate}
  \item Generate $n$ random individuals
  \item Calculate the fitness of each
  \item Randomly pair up individuals
  \item Recombine them to create two offspring
  \item Mutate each gene locus with a small probability based of the
  gene's standard deviation.
  \item Put the offspring in the new population ($2n$ large)
  \item Select the best $n$
  \item Go back to step 2, or stop.
\end{enumerate}

In this algorithm, the mutation happens in two stages: first the
standard deviation is mutated first, and then the gene is added to by
a random number based off the standard deviation.

The recombination operator is also slightly different; since each gene
in the child is the average of the parent gene:

\[
  (12 , 5 , 34 , 2), (10 , 7 , 30 , 4) \rightarrow (11 , 6 , 32 , 3)
\]

It is important to note the differences between selection and variation so we
can get the balance right:

\begin{itemize}
  \item Selection is responsible for keeping improvements in the population and
  not discarding them
  \item Very strong selection limits the variation and all individuals will be 
  the same
  \item Mutation and cross over are responsible for introducing variation and 
  will make the parameters/gene pool change
  \item Very strong variation results in good solutions, but they will be 
  replaced by random ones and not retained.
\end{itemize}

Many genetic algorithms progress at very slow rates, but then have short bursts
of very rapid progress. In general, it is not common for these algorithms to
have proven convergence properties, but some of them do when applied to simple
problems.

Deciding when to stop a genetic algorithm isn't always simple; stopping after a
predetermined number of generations requires lots of trial and error to get the
number right, waiting until a desired fitness is reached is okay if you know
what the optimal fitness is, but it may take an (infinitely) long amount of
time, and waiting for the fitness variations from one population to the next to
become stable isn't a good idea either because of the propensity for genetic
algorithms to progress very slowly then have a burst of progress.

% Lecture 5

\subsection{Genetic Programming}

With evolutionary programming, we optimised the parameters of the
input function in order to find the ones that produce the best
solution. On the other hand, genetic programming optimises the
function until it fits our purpose. This may seem like a strange
approach, but when you consider that any computer program is just a
big mathematical function, then genetic programming just becomes
automatic computer programming.

In this sense, programs evolve to become better at solving the problem
they are being evaluated against. If you think about it, programmers
have two `phrases' when to producing code; developing new code, and
editing existing code. This maps cleanly onto genetic programming,
since the algorithms first generate entirely new pseudo-random code,
and then mutate/cross it over with other code.

We can represent programs as trees, where internal nodes are functions
(which includes arithmetic/logical operations) with arguments, and
where leaf nodes correspond to constant values, functions with no
arguments, or inputs to the program.

The genetic algorithm must be given a set of functions and a set of
terminals that it can use to generate the tree. Example functions
include:

\begin{itemize}
\item Boolean functions (\texttt{AND}, \texttt{OR},\dots)
\item Arithmetic functions ($+$,$\%$,\dots)
\item Transcendental\footnote{A transcendental function is one that
cannot be described with a finite sequence of algebraic operations;
i.e. it transcends algebra.} functions (sin, cos, log,\dots)
\item Conditional statements (if, switch, etc)
\item Loops (while, for, etc)
\item Variable manipulation (set, etc)
\item Subroutines\footnote{Think of these like library calls. The
evolutionary algorithm is really just trying to string these
together.} (http-request, read-sensor, etc)
\end{itemize}

The set of terminal nodes is smaller, and can include
constants\footnote{Random constants are determined at the start of the
program (i.e. once per run).}, functions such as \texttt{rand()},
variables etc.

In order to create valid programs, the genetic algorithm should
implement some type checking to ensure that only valid inputs are
given to functions. This may include automatic type conversions to
increase the number of valid trees that can be generated.

The algorithm must also attempt to stop things
like \texttt{RuntimeException} occuring; i.e. the program must produce
some usable output. This can be done by having functions return a
default value if they fail with invalid inputs\footnote{Type checking
can't catch everything (though some functional languages have type
systems that integrate proof systems and can check all kinds of funky
stuff; Google for `dependent types'), divide by zero is an example of
this.}, or by making functions that fail score very low when they are
evaluated (so that they are selected out of the population).

\subsection{Cramer \& Koza's Genetic Programming}

As previously stated, the program is encoded as a tree, using
functions and terminals. Cramer and Koza's algorithm is as follows:

\begin{enumerate}
\item Initialise a random population of $n$ individuals
\item Calculate the fitness of each individual
\item Select two parents at random proportionally to their fitness and
reproduce them together, producing two children
\item Apply cross over with a certain probability
\item Apply mutation with a certain probability
\item Remove the parents
\item Loop back to step 2 unless satisfied.
\end{enumerate}

While the algorithm may seem well specified, there are a few
additional things we need to clear up; namely initialisation and
mutation/cross over.

Initialisation is done using one of two methods:

\begin{description}
\item \textbf{Full method}:\\ Choose a function node at random, and
either fill its children with terminal nodes if we have reached the
required depth, or loop and choose more function nodes.
\item \textbf{Grow method}:\\ Choose a function or a terminal node at
random, and fill its children with child nodes if the maximum depth
has been reached, or loop back and choose child or function nodes.
\end{description}

Both of these methods generate trees of up to a specified depth,
though the full method always generates a tree where all branches are
that depth, while the grow method generates branches \textit{up to}
that depth.

Mutation is applied to single individuals; a node is chosen at random
and is substituted with a randomly generated sub-tree. Different
implementations generate the sub-tree differently, some will maintain
the depth of the tree to avoid it growing too big or generate a tree
that can be used as input for the parent node (to avoid creating an
invalid tree post-mutation), while others have no such constraints.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/mutation}
\caption{Mutation of tree.}
\end{figure}

% Lecture 6

Cross over is when children inherit information from both their
parents. For our purposes, this involves making a tree with a sub-tree
from one parent and a sub-tree from another parent. The node within
each parent to cross over at is chosen at random (meaning that there
could be a case when most of the tree is inherited from one parent and
little of it is inherited from another.).

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/cross-over}
\caption{Cross over between two trees.}
\end{figure}

\subsection{Bloat}

Bloat is when programs created by genetic algorithms get larger and
larger trees without significant improvement to the fitness of the
program for the function it is trying to perform. This is undesirable,
since more memory is used, the expressions are evaluated slower and
the population evolves more slowly.

Bloat is caused by a variety of sources, such as when cross over is
very unbalanced, and through random mutations. Bloat often `survives'
and accumulates because it is neutral to fitness; many bloated
sub-trees will have no effect on the fitness (these are celled
introns).

Examples of introns include expressions like $x-x$,
$x~\&\&~\mathrm{true}$ etc (basically the identity functions).

Controlling bloat is hard; but we can do so in a few different ways:
\begin{itemize}
\item Enforce tree size and depth limits, maybe by returning a parent
of a tree if its child violates a limit.
\item Anti bloat operations could be implemented such as making sure
that the second subtree in cross over is small enough to not violate
size constraints, or by making sure that a mutated tree is of the same
size as the original one was.
\item Selection can be used to reduce bloat; the fitness of trees can
be reduced proportional to their size (parsimony pressure), or the
fitness of trees violating the size constraints could simply be set to
zero (the tarpeian method).
\end{itemize}

All evolutionary algorithms have a requirement for diversity and
variability in their population in order to evolve. When all
individuals in a population are similar, cross-over won't effect much
change and mutation is unlikely to move individuals far in the
`solution space'. Unfortunately, due to selection, populations tend to
evolve to be relatively uniform.

There are attempts to make evolutionary algorithms more similar to
nature in the sense that there can be multiple populations of the same
species in different areas with little mixing between them. Algorithms
mimicking this effect are called \textit{Distributed EA's}, and evolve
multiple populations in near-isolation (some mixing will occur) in
order to try and get heterogeneous sets of effective genes. Some swarm
algorithms are an example of this (multiple swarms can be present in
the solution space).

% Part Three

\section{Complex Networks}

Eva's notes and lectures are really very good, use her notes, not mine :)
